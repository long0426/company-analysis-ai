import json
import os
import datetime
import logging

from google.adk.agents.llm_agent import Agent
from google.adk.models.lite_llm import LiteLlm
from google.adk.tools.mcp_tool import McpToolset, StdioConnectionParams

from google.adk.tools import FunctionTool
from mcp.client.stdio import StdioServerParameters
from google.adk.models import LlmRequest
from google.genai import types

from .tools.instruction_reader import instruction_reader_tool
from .tools.yahoo_finance_tool import yahoo_finance_tool

# è¨­å®š Logger
logger = logging.getLogger("stock_agent")
logger.setLevel(logging.INFO)

# ... (omitting intermediate code, focus on load_instructions modification)

def load_instructions(
    instruction_dir=os.path.join(os.path.dirname(__file__), "instructions")
):
    """
    å‹•æ…‹è¼‰å…¥æ ¸å¿ƒ instruction æ–‡ä»¶ (Slim Mode)
    åƒ…è¼‰å…¥ agent_execution.md (æ†²æ³•) å’Œ 00_master_orchestrator (åœ°åœ–)ã€‚
    å…¶é¤˜è©³ç´°æŒ‡å—éœ€é€é read_instruction_manual å·¥å…·æŒ‰éœ€è®€å–ã€‚
    """
    instructions = []
    
    if not os.path.exists(instruction_dir):
        logger.warning(f"Warning: {instruction_dir} not found.")
        return "You are a professional Stock Analyst Agent."
    
    # 1. è¼‰å…¥ agent_execution.md (åŸ·è¡Œé…ç½®/æ†²æ³•)
    exec_config = os.path.join(instruction_dir, "agent_execution.md")
    if os.path.exists(exec_config):
        try:
            with open(exec_config, "r", encoding="utf-8") as f:
                content = f.read()
                _, content = parse_frontmatter(content)
                instructions.append(content)
            logger.info("âœ… Loaded Core: agent_execution.md")
        except Exception as e:
            logger.error(f"âš ï¸ Failed to load agent_execution.md: {e}")
            
    # 2. è¼‰å…¥ 00_master_orchestrator_*.md (ä¸»æ§ç·¨æ’å™¨/åœ°åœ–)
    # å°‹æ‰¾ 00_ é–‹é ­çš„æ–‡ä»¶
    master_file = None
    for f in os.listdir(instruction_dir):
        if f.startswith("00_") and f.endswith(".md"):
            master_file = f
            break
            
    if master_file:
        try:
            filepath = os.path.join(instruction_dir, master_file)
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()
                _, content = parse_frontmatter(content)
                instructions.append(content)
            logger.info(f"âœ… Loaded Core: {master_file}")
        except Exception as e:
            logger.error(f"âš ï¸ Failed to load {master_file}: {e}")
            
    # 3. æ·»åŠ å‹•æ…‹è¼‰å…¥èªªæ˜èˆ‡å¯ç”¨æ¨¡çµ„æ¸…å–®
    instructions.append("""
---
# ğŸ“š çŸ¥è­˜åº«ä½¿ç”¨èªªæ˜ (Dynamic Context Loading)

ç‚ºäº†ä¿æŒæ€ç¶­æ¸…æ™°ï¼Œç³»çµ±**æœªè¼‰å…¥**æ‰€æœ‰çš„è©³ç´°å¯«ä½œæŒ‡å— (Writing Guides)ã€‚
æ‚¨å¿…é ˆåœ¨åŸ·è¡Œç‰¹å®šä»»å‹™å‰ï¼Œä½¿ç”¨å·¥å…· `read_instruction_manual` æŸ¥é–±å°æ‡‰çš„æ“ä½œæ‰‹å†Šã€‚

**å¯ç”¨æ¨¡çµ„/æ‰‹å†Šæ¸…å–® (Available Manuals):**
""")
    
    # å‹•æ…‹ç”Ÿæˆæ¨¡çµ„æ¸…å–® (ç¾åœ¨åŒ…å«å¾ Markdown è§£æå‡ºçš„ description)
    modules = extract_modules_from_instructions(instruction_dir)
    for module_id, info in sorted(modules.items(), key=lambda x: x[1]['order']):
        instructions.append(f"### {info['name']} (ID: `{module_id}`)")
        # é¡¯ç¤ºé—œéµå­—/åˆ¥å
        if info['aliases']:
            instructions.append(f"- **é—œéµå­—**: {', '.join(info['aliases'][:3])}")
        # é¡¯ç¤ºè§£æå‡ºçš„ç”¨é€”èªªæ˜ (é€™æ˜¯é—œéµï¼Œæ›¿ä»£äº†åŸæœ¬ç¡¬ç·¨ç¢¼çš„å»ºè­°)
        if info['description']:
            instructions.append(f"- **ç”¨é€”/æ™‚æ©Ÿ**: {info['description']}")
        instructions.append("") # ç©ºè¡Œåˆ†éš”

    instructions.append("**è«‹é¤Šæˆã€Œå…ˆæŸ¥æ›¸ï¼Œå†åšäº‹ã€çš„ç¿’æ…£ï¼Œç¢ºå¯¦æŸ¥é–±å°æ‡‰æ‰‹å†Šä»¥ç¬¦åˆè¦ç¯„ï¼**")

    result = "\n\n".join(instructions)
    
    # [DEBUG] Log System Prompt to file
    try:
        log_dir = os.path.join(instruction_dir, "../logs")
        os.makedirs(log_dir, exist_ok=True)
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = os.path.join(log_dir, f"debug_system_prompt_{timestamp}.md")
        with open(log_file, "w", encoding="utf-8") as f:
            f.write(result)

        logger.info(f"ğŸ“ System Prompt logged to: {log_file}")
    except Exception as e:
        logger.error(f"âš ï¸ Failed to log system prompt: {e}")

    return result


# ... (omitting intermediate code) ...


def reload_agent():
    """
    é‡æ–°è¼‰å…¥ Agentï¼ˆé–‹ç™¼ç’°å¢ƒä½¿ç”¨ï¼‰
    
    Returns:
        Agent: æ–°çš„ Agent å¯¦ä¾‹
    """
    global root_agent
    
    print("\n" + "="*60)
    print("ğŸ”„ Reloading Agent with fresh instructions...")
    print("="*60 + "\n")
    
    # è¼‰å…¥ MCP å·¥å…·
    mcp_tools = load_mcp_tools()
    
    # é‡æ–°å‰µå»º Agent
    root_agent = Agent(
        model=LiteLlm(model="azure/gpt-4o"),
        name="stock_analyst",
        description="""
        æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„æŠ•è³‡åˆ†æå¸«ã€‚
    
        **å·¥ä½œæ¨¡å¼è®Šæ›´é€šçŸ¥**ï¼š
        ç‚ºäº†æå‡æº–ç¢ºåº¦ï¼Œè©³ç´°çš„å¯«ä½œæŒ‡å—ä¸å†é å…ˆè¼‰å…¥ã€‚
        æ‚¨å¿…é ˆåˆ©ç”¨ `read_instruction_manual` å·¥å…·ï¼Œæ¡å–ã€ŒæŒ‰éœ€æŸ¥é–± (Just-in-Time Learning)ã€çš„ç­–ç•¥ã€‚
        
        **é»˜èªè¡Œç‚º**ï¼š
        1. æ”¶åˆ°è‚¡ç¥¨ä»£è™Ÿ (å¦‚ TSMC)ã€‚
        2. é–±è®€ `00_master_orchestrator.md` (å·²è¼‰å…¥) äº†è§£æ•´é«”æµç¨‹ã€‚
        3. **Action Phase**: èª¿ç”¨ `get_stock_info` ç²å–æ•¸æ“šã€‚
        4. **Learning Phase**: èª¿ç”¨ `read_instruction_manual('part_a_writing_guide')` ç­‰å·¥å…·ï¼Œè¤‡ç¿’å¯«ä½œè¦ç¯„ã€‚
        5. **Execution Phase**: åš´æ ¼æŒ‰ç…§å‰›è®€åˆ°çš„è¦ç¯„èˆ‡ `agent_execution.md` çš„è¦æ±‚æ’°å¯«å ±å‘Šã€‚
        
        è«‹åš´æ ¼éµå¾ª agent_execution.md èˆ‡ master_orchestrator.md ä¸­å®šç¾©çš„å®Œæ•´åŸ·è¡Œæµç¨‹ã€‚
        é‡åˆ°è‚¡ç¥¨ä»£è™Ÿè¼¸å…¥æ™‚ï¼Œ**åš´ç¦**åƒ…çµ¦å‡ºç°¡å–®æ‘˜è¦ï¼Œå¿…é ˆç”Ÿæˆå®Œæ•´å ±å‘Šã€‚
        æ‰€æœ‰è¼¸å‡ºå¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
        """,
        tools=mcp_tools + [yahoo_finance_tool, instruction_reader_tool],
        static_instruction=get_instructions(force_reload=True),
    )
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ”„ Reloading Agent with fresh instructions...")
    logger.info("="*60 + "\n")
    return root_agent


def load_mcp_tools(config_path="mcp_servers.json"):
    """
    å¾é…ç½®æª”æ¡ˆè¼‰å…¥ MCP å·¥å…·ã€‚

    Args:
        config_path (str): MCP é…ç½®æª”æ¡ˆè·¯å¾‘

    Returns:
        list: MCP å·¥å…·æ¸…å–®
    """
    tools = []

    # ç¢ºä¿ä½¿ç”¨çµ•å°è·¯å¾‘
    if not os.path.isabs(config_path):
        # å¾å°ˆæ¡ˆæ ¹ç›®éŒ„å°‹æ‰¾é…ç½®æª”æ¡ˆ
        project_root = os.path.dirname(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        )
        config_path = os.path.join(project_root, config_path)

    logger.info(f"ğŸ” Looking for MCP config at: {config_path}")

    if not os.path.exists(config_path):
        logger.warning(f"Warning: {config_path} not found. Skipping MCP tools.")
        return tools

    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)

        logger.info(f"ğŸ“„ Found {len(config.get('mcpServers', {}))} MCP server(s) in config")

        for name, params in config.get("mcpServers", {}).items():
            logger.info(f"Loading MCP tool: {name}")
            logger.info(f"  Command: {params.get('command')} {' '.join(params.get('args', []))}")

            try:
                stdio_params = StdioConnectionParams(
                    server_params=StdioServerParameters(
                        command=params.get("command"),
                        args=params.get("args", []),
                        env=params.get("env"),
                    )
                )
                tools.append(
                    McpToolset(
                        connection_params=stdio_params,
                        tool_name_prefix=params.get("tool_name_prefix", f"{name}_"),
                    )
                )
                logger.info(f"âœ… Loaded: {name}")

            except Exception as tool_error:
                logger.error(f"âŒ Failed to load {name}: {tool_error}")
                import traceback
                traceback.print_exc()
                # ç¹¼çºŒè¼‰å…¥å…¶ä»–å·¥å…·ï¼Œä¸è¦å› ç‚ºä¸€å€‹å¤±æ•—å°±å…¨éƒ¨ä¸­æ–·

    except Exception as e:
        logger.error(f"âš ï¸ Error loading MCP config: {e}")
        import traceback
        traceback.print_exc()

    logger.info(f"ğŸ“Š Total MCP tools loaded: {len(tools)}")
    return tools


# ============================================================================
# Instruction Loader with Dynamic Module Detection
# ============================================================================

import re
from typing import Dict, Any, Tuple

try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False
    logger.warning("âš ï¸ Warning: pyyaml not installed. Frontmatter parsing disabled.")
    logger.warning("   Install with: uv add pyyaml")


def parse_frontmatter(content: str) -> Tuple[Dict[str, Any], str]:
    """
    è§£æ YAML frontmatter
    
    Args:
        content: æ–‡ä»¶å…§å®¹
    
    Returns:
        (frontmatter_dict, content_without_frontmatter)
    """
    if not YAML_AVAILABLE:
        return {}, content
    
    pattern = r'^---\s*\n(.*?)\n---\s*\n(.*)$'
    match = re.match(pattern, content, re.DOTALL)
    
    if match:
        yaml_content, markdown_content = match.groups()
        try:
            frontmatter = yaml.safe_load(yaml_content)
            return frontmatter or {}, markdown_content
        except yaml.YAMLError as e:
            logger.warning(f"âš ï¸ Warning: Failed to parse YAML frontmatter: {e}")
            return {}, content
    
    return {}, content


def parse_markdown_metadata(content: str) -> Dict[str, Any]:
    """
    å¾ Markdown å…§å®¹ä¸­è§£æåˆ—è¡¨å¼å…ƒæ•¸æ“š (Fallback for missing YAML)
    
    å°‹æ‰¾é¡ä¼¼ä»¥ä¸‹çš„æ¨¡å¼:
    - **key**: value
    """
    metadata = {}
    # æŸ¥æ‰¾æ‰€æœ‰ - **Key**: Value æ ¼å¼çš„è¡Œ (åªåœ¨æ–‡ä»¶å‰ 50 è¡ŒæŸ¥æ‰¾)
    lines = content.split('\n')[:50]
    
    for line in lines:
        match = re.match(r'^[-*]\s+\*\*([^*\n]+)\*\*:\s*(.+)$', line.strip())
        if match:
            key = match.group(1).lower()
            value = match.group(2).strip()
            
            # æ˜ å°„å¸¸è¦‹éµååˆ°æ¨™æº–å­—æ®µ
            if key in ['ç”¨é€”', 'purpose', 'description', 'èªªæ˜']:
                metadata['description'] = value
            elif key in ['name', 'åç¨±', 'title']:
                metadata['name'] = value
            elif key in ['alias', 'aliases', 'åˆ¥å']:
                metadata['aliases'] = [a.strip() for a in value.split(',')]
                
    return metadata


def extract_modules_from_instructions(instruction_dir: str) -> Dict[str, Dict]:
    """
    å¾ instructions ç›®éŒ„è‡ªå‹•è­˜åˆ¥å¯ç”¨æ¨¡çµ„
    å„ªå…ˆè§£æ YAML frontmatterï¼Œå¤±æ•—å‰‡å˜—è©¦è§£æ Markdown metadata
    """
    modules = {}
    # åŒ¹é…: æ•¸å­—_æ¨¡çµ„ID_å…¶ä»–.md
    pattern = re.compile(r'(\d+)_([a-z_]+)(?:_(.+))?\.md')
    
    # é è¨­åˆ¥ååº« (åƒ…ä½œå¾Œå‚™)
    DEFAULT_ALIASES = {
        'master_orchestrator': ['ä¸»æ§ç·¨æ’å™¨', 'orchestrator'],
        'include_all': ['å…¨éƒ¨', 'all'],
    }
    
    if not os.path.exists(instruction_dir):
        return modules
    
    for filename in os.listdir(instruction_dir):
        if filename == 'agent_execution.md':
            continue
            
        match = pattern.match(filename)
        if match:
            order = match.group(1)
            module_id = match.group(2)
            filepath = os.path.join(instruction_dir, filename)
            
            module_info = {}
            description = ""
            
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # 1. å˜—è©¦ YAML è§£æ
                frontmatter, _ = parse_frontmatter(content)
                if frontmatter and 'module' in frontmatter:
                    module_info = frontmatter.get('module', {})
                    description = module_info.get('description', '')
                else:
                    # 2. å˜—è©¦ Markdown è§£æ
                    md_metadata = parse_markdown_metadata(content)
                    module_info = md_metadata
                    description = md_metadata.get('description', '')
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Warning: Failed to read {filename}: {e}")
            
            # ç”Ÿæˆé è¨­åç¨±
            default_name = module_id.replace('_', ' ').title()
            
            # åˆä½µåˆ¥å (é è¨­åˆ¥å + æ–‡ä»¶ä¸­å®šç¾©çš„åˆ¥å)
            aliases = DEFAULT_ALIASES.get(module_id, [])
            if 'aliases' in module_info:
                if isinstance(module_info['aliases'], list):
                    aliases.extend(module_info['aliases'])
                else:
                    aliases.append(module_info['aliases'])
            
            modules[module_id] = {
                'file': filename,
                'order': int(order),
                'path': filepath,
                'name': module_info.get('name', default_name),
                'aliases': list(set(aliases)), # å»é‡
                'word_count': module_info.get('word_count', {}),
                'description': description, # é€™æ˜¯é—œéµï¼Œä¾†è‡ª "ç”¨é€”"
                'optional': module_info.get('optional', True),
            }
    
    return modules


def load_instructions(
    instruction_dir=os.path.join(os.path.dirname(__file__), "instructions")
):
    """
    å‹•æ…‹è¼‰å…¥æ‰€æœ‰ instruction æ–‡ä»¶
    
    Args:
        instruction_dir: instruction æ–‡ä»¶ç›®éŒ„è·¯å¾‘
    
    Returns:
        str: çµ„åˆå¾Œçš„å®Œæ•´æŒ‡ä»¤å…§å®¹
    """
    instructions = []
    
    # æª¢æŸ¥æŒ‡ä»¤ç›®éŒ„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(instruction_dir):
        logger.warning(f"Warning: {instruction_dir} not found.")
        return """
        You are a professional Stock Analyst Agent.
        Your goal is to provide insightful analysis of stock information provided by the user.
        
        **Constraints:** 
        1. You MUST output your final response in **Traditional Chinese (ç¹é«”ä¸­æ–‡)**.
        """
    
    # 1. å„ªå…ˆè¼‰å…¥ agent_execution.md (åŸ·è¡Œé…ç½®)
    exec_config = os.path.join(instruction_dir, "agent_execution.md")
    if os.path.exists(exec_config):
        try:
            with open(exec_config, "r", encoding="utf-8") as f:
                content = f.read()
                # ç§»é™¤ frontmatter (å¦‚æœæœ‰)
                _, content = parse_frontmatter(content)
                instructions.append(content)
            logger.info("âœ… Loaded: agent_execution.md")
        except Exception as e:
            logger.error(f"âš ï¸ Failed to load agent_execution.md: {e}")
    
    # 2. è¼‰å…¥æ‰€æœ‰å…¶ä»– .md æ–‡ä»¶ (æŒ‰æª”åæ’åº)
    files = sorted([
        f for f in os.listdir(instruction_dir) 
        if f.endswith(".md") and f != "agent_execution.md"
    ])
    
    for filename in files:
        filepath = os.path.join(instruction_dir, filename)
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()
                # ç§»é™¤ frontmatterï¼Œåƒ…ä¿ç•™ markdown å…§å®¹
                _, content = parse_frontmatter(content)
                instructions.append(content)
            logger.info(f"âœ… Loaded: {filename}")
        except Exception as e:
            logger.error(f"âš ï¸ Failed to load {filename}: {e}")
    
    # 3. è‡ªå‹•ç”Ÿæˆæ¨¡çµ„æ¸…å–®èªªæ˜ï¼ˆåŒ…å«åˆ¥åè³‡è¨Šï¼‰
    modules = extract_modules_from_instructions(instruction_dir)
    module_info = "\n\n---\n\n# ğŸ” ç³»çµ±å·²è­˜åˆ¥çš„æ¨¡çµ„\n\n"
    module_info += "æœ¬ç³»çµ±è‡ªå‹•æƒæ instructions ç›®éŒ„ä¸¦è§£æ frontmatterï¼Œè­˜åˆ¥ä»¥ä¸‹å¯ç”¨æ¨¡çµ„:\n\n"
    
    for module_id, info in sorted(modules.items(), key=lambda x: x[1]['order']):
        module_info += f"## {info['name']} ({module_id})\n\n"
        module_info += f"- **æ–‡ä»¶**: {info['file']}\n"
        if info['description']:
            module_info += f"- **èªªæ˜**: {info['description']}\n"
        if info['aliases']:
            aliases_str = 'ã€'.join(f'"{a}"' for a in info['aliases'])
            module_info += f"- **åˆ¥å**: {aliases_str}\n"
        if info['word_count']:
            wc = info['word_count']
            if 'min' in wc and 'max' in wc:
                module_info += f"- **å­—æ•¸**: {wc['min']}-{wc['max']} å­—\n"
        module_info += "\n"
    
    module_info += "ç”¨æˆ¶å¯ä»¥ä½¿ç”¨æ¨¡çµ„åç¨±æˆ–ä»»ä½•åˆ¥åä¾†é¸æ“‡æ€§ç”Ÿæˆä»»æ„æ¨¡çµ„çµ„åˆã€‚\n"
    
    logger.info(f"\nğŸ“Š Total modules detected: {len(modules)}")
    for module_id, info in sorted(modules.items(), key=lambda x: x[1]['order']):
        aliases = ', '.join(info['aliases'][:3])  # é¡¯ç¤ºå‰3å€‹åˆ¥å
        logger.info(f"   - {info['name']}: {aliases}...")
    
    # çµ„åˆæ‰€æœ‰å…§å®¹
    result = "\n\n---\n\n".join(instructions) + module_info
    logger.info(f"\nğŸ“ Total instruction length: {len(result)} characters")
    
    return result




# ============================================================================
# Agent Configuration with Caching and Reload Support
# ============================================================================

# å…¨å±€ç·©å­˜
_instruction_cache = None
_instruction_mtime = {}  # å„²å­˜æ–‡ä»¶çš„ä¿®æ”¹æ™‚é–“


def get_instruction_files_mtime(instruction_dir):
    """ç²å–æ‰€æœ‰ instruction æ–‡ä»¶çš„ä¿®æ”¹æ™‚é–“"""
    mtimes = {}
    if not os.path.exists(instruction_dir):
        return mtimes
    
    for filename in os.listdir(instruction_dir):
        if filename.endswith('.md'):
            filepath = os.path.join(instruction_dir, filename)
            try:
                mtimes[filename] = os.path.getmtime(filepath)
            except OSError:
                pass
    
    return mtimes


def has_instructions_changed(instruction_dir):
    """æª¢æŸ¥ instruction æ–‡ä»¶æ˜¯å¦æœ‰è®ŠåŒ–"""
    global _instruction_mtime
    
    current_mtimes = get_instruction_files_mtime(instruction_dir)
    
    # ç¬¬ä¸€æ¬¡æª¢æŸ¥æˆ–æ–‡ä»¶æ•¸é‡è®ŠåŒ–
    if not _instruction_mtime or len(_instruction_mtime) != len(current_mtimes):
        return True
    
    # æª¢æŸ¥æ¯å€‹æ–‡ä»¶çš„ä¿®æ”¹æ™‚é–“
    for filename, mtime in current_mtimes.items():
        if filename not in _instruction_mtime or _instruction_mtime[filename] != mtime:
            return True
    
    return False


def get_instructions(
    force_reload: bool = False,
    auto_detect_changes: bool = False,
    instruction_dir=None
):
    """
    ç²å– instructionsï¼ˆå¸¶ç·©å­˜æ”¯æŒï¼‰
    
    Args:
        force_reload: å¼·åˆ¶é‡æ–°è¼‰å…¥ï¼Œå¿½ç•¥ç·©å­˜
        auto_detect_changes: è‡ªå‹•åµæ¸¬æ–‡ä»¶è®ŠåŒ–ï¼Œè‹¥æœ‰è®ŠåŒ–å‰‡é‡æ–°è¼‰å…¥
        instruction_dir: instruction æ–‡ä»¶ç›®éŒ„è·¯å¾‘
    
    Returns:
        str: çµ„åˆå¾Œçš„å®Œæ•´æŒ‡ä»¤å…§å®¹
    """
    global _instruction_cache, _instruction_mtime
    
    if instruction_dir is None:
        instruction_dir = os.path.join(os.path.dirname(__file__), "instructions")
    
    # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è¼‰å…¥
    need_reload = (
        force_reload or 
        _instruction_cache is None or
        (auto_detect_changes and has_instructions_changed(instruction_dir))
    )
    
    if need_reload:
        logger.info("ğŸ”„ Reloading instructions...")
        _instruction_cache = load_instructions(instruction_dir)
        _instruction_mtime = get_instruction_files_mtime(instruction_dir)
    else:
        logger.info("âœ… Using cached instructions")
    
    return _instruction_cache


def reload_agent():
    """
    é‡æ–°è¼‰å…¥ Agentï¼ˆé–‹ç™¼ç’°å¢ƒä½¿ç”¨ï¼‰
    
    Returns:
        Agent: æ–°çš„ Agent å¯¦ä¾‹
    """
    global root_agent
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ”„ Reloading Agent with fresh instructions...")
    logger.info("="*60 + "\n")
    
    # è¼‰å…¥ MCP å·¥å…·
    mcp_tools = load_mcp_tools()
    
    # é‡æ–°å‰µå»º Agent
    root_agent = Agent(
        model=LiteLlm(model="azure/gpt-4o"),
        name="stock_analyst",
        description="""
        æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„æŠ•è³‡åˆ†æå¸«ã€‚
        
        ç³»çµ±æœƒè‡ªå‹•è­˜åˆ¥å¯ç”¨çš„åˆ†ææ¨¡çµ„ï¼ˆé€šéè§£æ instruction æ–‡ä»¶çš„ frontmatterï¼‰ï¼Œ
        ç”¨æˆ¶å¯ä»¥ä½¿ç”¨æ¨¡çµ„åç¨±æˆ–åˆ¥åé¸æ“‡éœ€è¦çš„æ¨¡çµ„çµ„åˆã€‚
        
        é»˜èªè¡Œç‚ºï¼šç”Ÿæˆæ‰€æœ‰å·²è­˜åˆ¥çš„æ¨¡çµ„
        éˆæ´»é¸æ“‡ï¼šç”¨æˆ¶å¯æŒ‡å®šç‰¹å®šæ¨¡çµ„ï¼ˆä¾‹å¦‚:"åªè¦åˆ¸å•†å ±å‘Š"ã€"å¿½ç•¥é™„éŒ„"ç­‰ï¼‰
        
        è«‹åš´æ ¼éµå¾ª system instructions ä¸­å®šç¾©çš„å®Œæ•´åŸ·è¡Œæµç¨‹èˆ‡æ ¼å¼è¦æ±‚ã€‚
        æ‰€æœ‰è¼¸å‡ºå¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
        
        æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·ï¼š
        - get_stock_info: ç²å–è‚¡ç¥¨åŸºæœ¬è³‡è¨Šã€è²¡å‹™æ•¸æ“šå’Œæ–°è
        - search_*: æœå°‹ç¶²è·¯ä¸Šçš„è²¡ç¶“æ–°èã€ç”¢æ¥­åˆ†ææ–‡ç« 
        - fetch_*: æŠ“å–ç‰¹å®šç¶²é çš„è©³ç´°å…§å®¹
        """,
        tools=mcp_tools + [yahoo_finance_tool],
        static_instruction=get_instructions(force_reload=True),
    )
    
    logger.info("\nâœ… Agent reloaded successfully!\n")
    return root_agent


# ============================================================================
# Agent Factory & Pipeline Execution (New Architecture)
# ============================================================================

from typing import TypedDict, List, Optional

class AnalysisContext(TypedDict):
    """åˆ†æä¸Šä¸‹æ–‡ï¼šåœ¨æµæ°´ç·šå„éšæ®µé–“å‚³éçš„ç‹€æ…‹"""
    ticker: str
    company_name: str
    report_date: str
    analysis_start_date: str
    analysis_end_date: str
    data_source: str
    analysis_angles: List[str]
    report_type: str
    # å„²å­˜ä¸­é–“ç”¢ç‰©
    part_a_content: Optional[str]
    part_b_content: Optional[str]
    appendix_content: Optional[str]

def resolve_instruction_file(instruction_dir: str, filename: str) -> str:
    """
    å‹•æ…‹è§£æ Instruction æª”æ¡ˆè·¯å¾‘ (æ”¯æ´ç‰ˆæœ¬è™Ÿè‡ªå‹•åŒ¹é…)
    
    è¦å‰‡ï¼š
    1. è‹¥ç²¾ç¢ºåŒ¹é…åˆ°æª”æ¡ˆï¼Œç›´æ¥å›å‚³ã€‚
    2. è‹¥ç„¡ï¼Œå‰‡å˜—è©¦åŒ¹é… "base_name" + "_v*.md"ã€‚
    3. å–å­—æ¯æ’åºæœ€å¤§çš„ç‰ˆæœ¬ (latest version)ã€‚
    """
    # 1. ç›´æ¥åŒ¹é…
    exact_path = os.path.join(instruction_dir, filename)
    if os.path.exists(exact_path):
        return exact_path
        
    # 2. æ¨¡ç³ŠåŒ¹é…ç‰ˆæœ¬
    base_name = filename.replace('.md', '')
    candidates = []
    
    if os.path.exists(instruction_dir):
        for f in os.listdir(instruction_dir):
            if f.startswith(base_name) and f.endswith(".md"):
                candidates.append(f)
                
    if candidates:
        # æ’åºå–æœ€æ–°ç‰ˆ (v3.4.0 > v3.3.0)
        best_match = sorted(candidates)[-1]
        logger.info(f"ğŸ”— Resolved '{filename}' to '{best_match}'")
        return os.path.join(instruction_dir, best_match)
        
    logger.warning(f"âš ï¸ Instruction file not found: {filename}")
    return exact_path # Return original path to let it fail gracefully later

def create_stage_agent(
    stage_name: str,
    instruction_files: List[str],
    description_override: str = "",
    tools: List[any] = None,
    include_base_instructions: bool = True
) -> Agent:
    """
    Agent Factory: å‰µå»ºç‰¹å®šéšæ®µå°ˆç”¨çš„è¼•é‡ç´š Agent
    
    Args:
        stage_name: éšæ®µåç¨± (ç”¨æ–¼ Log)
        instruction_files: éœ€è¦è¼‰å…¥çš„æŒ‡ä»¤æª”æ¡ˆåˆ—è¡¨ (e.g., ['04_part_a_writing_guide.md'])
        description_override: è©² Agent çš„å°ˆå±¬è§’è‰²èªªæ˜
        tools: è©² Agent å¯ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨
        include_base_instructions: æ˜¯å¦åŒ…å« agent_execution.md (é è¨­ True)
    """
    logger.info("ğŸ­ Creating Agent for " + stage_name + "...")
    
    # 1. åŸºç¤æŒ‡ä»¤ (å¯é¸åŒ…å« agent_execution.md)
    base_instructions = []
    instruction_dir = os.path.join(os.path.dirname(__file__), "instructions")
    
    if include_base_instructions:
        # è®€å– agent_execution.md
        exec_config = os.path.join(instruction_dir, "agent_execution.md")
        if os.path.exists(exec_config):
            with open(exec_config, "r", encoding="utf-8") as f:
                content = f.read()
                _, content = parse_frontmatter(content)
                base_instructions.append(content)
            
    # 2. éšæ®µç‰¹å®šæŒ‡ä»¤
    for fname in instruction_files:
        if not fname.endswith(".md"): 
            fname += ".md"
            
        # [Fix] ä½¿ç”¨å‹•æ…‹è§£æï¼Œä¸å†å¯«æ­»ç‰ˆæœ¬è™Ÿ
        fpath = resolve_instruction_file(instruction_dir, fname)
        
        if os.path.exists(fpath):
            with open(fpath, "r", encoding="utf-8") as f:
                content = f.read()
                _, content = parse_frontmatter(content)
                base_instructions.append(content)
        else:
            logger.warning(f"âš ï¸ Instruction file not found: {fname}")

    combined_instructions = "\n\n---\n\n".join(base_instructions)
    
    # 3. å‰µå»º Agent
    return Agent(
        model=LiteLlm(model="azure/gpt-4o"),
        name=f"stock_analyst_{stage_name.replace(' ', '_')}",
        description=description_override or "æ‚¨æ˜¯å°ˆæ¥­çš„è‚¡ç¥¨åˆ†æå¸«ï¼Œè«‹å°ˆæ³¨æ–¼ç•¶å‰çš„åˆ†æéšæ®µã€‚",
        tools=tools or [],
        static_instruction=combined_instructions,
        include_contents='none'
    )


async def _execute_agent_and_get_text(agent: Agent, prompt: str, parent_context=None) -> str:
    """
    Helper function to execute an Agent's logic using its underlying model.
    We bypass `agent.run_async` because it is strictly tied to the framework's Event/Session loop
    and doesn't allow easy injection of new prompts for sub-tasks (Stage 1/2/3).
    """
    response_text = ""
    try:
        # Construct messages manually
        # ADK LiteLlm uses LlmRequest logic
        # structure: contents=[{'role': '...', 'parts': [{'text': '...'}]}]
        
        contents = []
        
        # Combine Description (Agent Persona + Date) and Static Instructions
        full_system_prompt = ""
        if agent.description:
            full_system_prompt += f"{agent.description}\n\n"
        
        if agent.static_instruction:
            full_system_prompt += agent.static_instruction

        if full_system_prompt:
            # logger.info(f"ğŸ› [DEBUG] System Prompt for {agent.name}:\n{full_system_prompt}\n" + "="*50)
            
            # [Fix] Write to file to bypass terminal limits (Append mode)
            try:
                with open("latest_debug_prompt.txt", "a", encoding="utf-8") as f:
                    f.write(f"\n\n{'='*50}\n")
                    f.write(f"Agent: {agent.name}\nTimestamp: {datetime.datetime.now()}\n{'='*20}\n\n")
                    f.write(full_system_prompt)
            except Exception as e:
                logger.error(f"Failed to write debug file: {e}")

            contents.append({
                "role": "system",
                "parts": [{"text": full_system_prompt}]
            })
            
        contents.append({
            "role": "user",
            "parts": [{"text": prompt}]
        })
        
        logger.info(f"âš¡ï¸ Executing {agent.name} via direct model call (Prompt len: {len(prompt)})")
        
        model = agent.model
        
        # Check for generate_content_async (ADK Model API)
        if hasattr(model, 'generate_content_async'):
             # Create LlmRequest
             # Note: agent.model.model holds the model name (e.g. "azure/gpt-4o")
             request = LlmRequest(
                 model=getattr(model, 'model', "azure/gpt-4o"),
                 contents=contents,
                 config=types.GenerateContentConfig(max_output_tokens=8192)
             )
             
             async for response in model.generate_content_async(request):
                 # response is LlmResponse
                 chunk_text = ""
                 
                 # Capture Token Usage
                 if hasattr(response, 'usage_metadata') and response.usage_metadata:
                     token_info = response.usage_metadata
                     logger.info(f"ğŸ“Š [Token Usage] Agent: {agent.name} | Input: {getattr(token_info, 'prompt_token_count', 'N/A')} | Output: {getattr(token_info, 'candidates_token_count', 'N/A')} | Total: {getattr(token_info, 'total_token_count', 'N/A')}")
                 
                 # Check content/text fields
                 val = None
                 if hasattr(response, 'content') and response.content:
                     val = response.content
                 elif hasattr(response, 'text') and response.text:
                     val = response.text
                 
                 # Process value
                 if val:
                     if isinstance(val, str):
                         chunk_text = val
                     # Handle ADK Content object (has 'parts')
                     elif hasattr(val, 'parts') and val.parts:
                         # parts is a list of Part objects
                         if hasattr(val.parts[0], 'text'):
                             chunk_text = val.parts[0].text
                         elif isinstance(val.parts[0], dict) and 'text' in val.parts[0]:
                             chunk_text = val.parts[0]['text']
                             
                 # Fallback: check candidates if top-level content was empty
                 if not chunk_text and hasattr(response, 'candidates') and response.candidates:
                      parts = response.candidates[0].content.parts
                      if parts:
                          if hasattr(parts[0], 'text'):
                              chunk_text = parts[0].text
                          elif isinstance(parts[0], dict) and 'text' in parts[0]:
                              chunk_text = parts[0]['text']

                 if chunk_text:
                     response_text += chunk_text

        # Fallback to completion (if somehow generate_content_async is missing but completion exists)
        elif hasattr(model, 'completion'): 
             # OpenAI format
             messages = [{"role": "system", "content": agent.static_instruction}, {"role": "user", "content": prompt}]
             response = await model.completion(messages=messages)
             if isinstance(response, str):
                 response_text = response
             elif hasattr(response, 'choices'):
                 response_text = response.choices[0].message.content
        else:
             logger.error(f"âŒ Unknown model interface: {type(agent.model)}")
             return "Error: Unknown model interface"
             
    except Exception as e:
        logger.error(f"âŒ Error executing agent model: {e}")
        import traceback
        traceback.print_exc()
        raise e
        
    return response_text
def _validate_stage_0_json(data: dict) -> Tuple[bool, str]:
    """
    [Code Validation] å¯¦ä½œ `07_quality_checklist` ä¸­çš„ "Checklist 0.1: åˆ†æå‰ç½®æª¢æŸ¥"
    
    å³ä¾¿é©—è­‰é‚è¼¯æ˜¯å¯«æ­»çš„ (Python Regex)ï¼Œå…¶è¦ç¯„ä¾†æºä»æ‡‰ç‚º Quality Checklist æ–‡ä»¶ï¼Œ
    ä»¥ç¢ºä¿å–®ä¸€çœŸå¯¦ä¾†æº (Single Source of Truth)ã€‚
    """
    errors = []
    
    # 1. æª¢æŸ¥ Title æ ¼å¼
    title = data.get("report_title", "")
    # Regex: Start with #, contain (), and end with æŠ•è³‡åˆ†æå ±å‘Š
    # e.g., "# Apple Inc. (AAPL) - æŠ•è³‡åˆ†æå ±å‘Š"
    title_pattern = r"^#\s+.*\s+\(.*\)\s+-\s+.*$"
    if not re.match(title_pattern, title):
        errors.append(f"âŒ Invalid 'report_title': '{title}'. Must match format '# Company (Ticker) - ...'")
        
    # 2. æª¢æŸ¥ TOC å®Œæ•´æ€§
    toc = data.get("table_of_contents", "")
    required_sections = ["Part A:", "Part B:", "Appendix", "ç›®éŒ„"]
    missing_sections = [sec for sec in required_sections if sec not in toc]
    if missing_sections:
        errors.append(f"âŒ Missing sections in 'table_of_contents': {missing_sections}")
        
    # [Clean Text Validation] æª¢æŸ¥æ˜¯å¦å«æœ‰ Markdown ç¬¦è™Ÿ
    # æª¢æŸ¥ **bold**
    if "**" in toc or "**" in title:
         errors.append(f"âŒ Markdown bold syntax '**' found. Use Clean Text format.")
    
    # æª¢æŸ¥ List Bullet '-' (æª¢æŸ¥æ¯ä¸€è¡Œé–‹é ­)
    for line in toc.split('\n'):
        if line.strip().startswith("- ") or line.strip().startswith("* "):
            errors.append(f"âŒ Markdown list syntax '- ' or '* ' found in TOC. Use indented numbers (e.g., '   1.1 ...').")
            break
        
    if errors:
        return False, "\n".join(errors)
        
    return True, "PASS"

async def _run_stage_0(user_request: str, tool_context=None) -> AnalysisContext:
    """Stage 0: åˆ†ææº–å‚™ (Context Gathering)"""
    logger.info("ğŸš€ Starting Stage 0: Context Gathering")
    
    # å·¥å…·ï¼šçµ¦äºˆ Context Reader å’Œ Search å·¥å…·
    stage_tools = load_mcp_tools() + [yahoo_finance_tool]
    
    agent = create_stage_agent(
        stage_name="stage_0_context",
        instruction_files=["00_stage_0_instruction.md", "02_data_source_selection.md", "03_analysis_framework_selector.md"],
        include_base_instructions=False, # âŒ ç¦æ­¢è¼‰å…¥ agent_execution.mdï¼Œé¿å…æ±¡æŸ“ Prompt
        description_override=f"""
        æ‚¨æ˜¯åˆ†ææµç¨‹çš„æŒ‡æ®å®˜ (Stage 0)ã€‚
        **ç•¶å‰ç³»çµ±æ—¥æœŸ**ï¼š{datetime.datetime.now().strftime('%Y-%m-%d')} (ä»¥æ­¤ç‚ºåŸºæº–è¨­å®šæ‰€æœ‰æ—¥æœŸ)
        
        ä»»å‹™ï¼š
        1. è§£æç”¨æˆ¶éœ€æ±‚ã€‚
        2. æ±ºå®šåˆ†ææœŸé–“ (Analysis Period) èˆ‡ æ•¸æ“šæˆªæ­¢æ—¥ã€‚
        3. é¸æ“‡æœ€åˆé©çš„æ•¸æ“šæºèˆ‡åˆ†æè¦–è§’ã€‚
        4. **æœ€å¾Œè¼¸å‡º**ï¼šå¿…é ˆ **åªè¼¸å‡º** ä¸€å€‹ JSON æ ¼å¼çš„ Analysis Contextã€‚
        
        ğŸ‘‰ **è«‹å‹™å¿…åš´æ ¼éµå®ˆ `00_stage_0_instruction` ä¸­çš„ã€Œæ¬„ä½è©³ç´°è¦æ±‚ã€**ã€‚
        (ç‰¹åˆ¥æ˜¯ `report_title` æ ¼å¼èˆ‡ `table_of_contents` å®Œæ•´åº¦)
        """ + """
        è¼¸å‡º JSON æ ¼å¼ç¯„ä¾‹ï¼š
        ```json
        {
            "ticker": "AMD",
            "company_name": "Advanced Micro Devices, Inc.",
            "report_date": "2026-01-13",
            "analysis_start_date": "2025-01-13",
            "analysis_end_date": "2026-01-13",
            "data_source": "Morningstar (Primary) + Yahoo Finance (Secondary)",
            "analysis_angles": ["ç«¶çˆ­æ ¼å±€", "ä¾›éœ€åˆ†æ", "æŠ€è¡“æ›¿ä»£é¢¨éšª"],
            "report_type": "æ¨™æº–æˆé•·æ¡†æ¶",
            "report_title": "# Advanced Micro Devices (AMD) - æŠ•è³‡åˆ†æå ±å‘Š",
            "table_of_contents": "ç›®éŒ„\\n\\n1. Part A: æ·±åº¦åˆ†æå ±å‘Š\\n   1.1 é‡è¦è¨Šæ¯\\n   1.2 è©•è«–åŠåˆ†æ\\n   1.3 ä¼°å€¼èˆ‡ç›®æ¨™åƒ¹\\n   1.4 æŠ•è³‡å»ºè­°\\n   1.5 æŠ•è³‡é¢¨éšª\\n\\n2. Part B: é‡é»æ‘˜è¦è¡¨æ ¼\\n   2.1 è²¡å‹™æ¦‚è¦\\n   2.2 ä¼°å€¼æŒ‡æ¨™\\n\\n3. é™„éŒ„ (Appendix)\\n   3.1 æ•¸æ“šä¾†æºè²æ˜\\n   3.2 å®šç¾©èˆ‡æ–¹æ³•è«–"
        }
        ```
        è«‹ç¢ºä¿æ‚¨çš„å›ç­”åƒ…åŒ…å« JSONï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
        """,
        tools=stage_tools
    )
    
    
    
    MAX_RETRIES = 3
    last_error = ""
    current_prompt = user_request

    for i in range(MAX_RETRIES):
        logger.info(f"ğŸ¤– Stage 0 Agent Executing (Attempt {i+1}/{MAX_RETRIES})...")
        response_text = await _execute_agent_and_get_text(agent, current_prompt, parent_context=tool_context)
        
        try:
            # JSON è§£æ
            clean_json = response_text.replace("```json", "").replace("```", "").strip()
            # ç°¡å–®çš„æ‹¬è™Ÿæå–é˜²è­·
            idx_start = clean_json.find('{')
            idx_end = clean_json.rfind('}')
            if idx_start != -1 and idx_end != -1:
                clean_json = clean_json[idx_start:idx_end+1]
                
            context_data = json.loads(clean_json)
            
            # é©—è­‰
            is_valid, validation_msg = _validate_stage_0_json(context_data)
            
            if is_valid:
                logger.info(f"âœ… Stage 0 JSON Validated & Passed.")
                # è£œå…¨æ¬„ä½ä¸¦å›å‚³
                return {
                    "ticker": context_data.get("ticker", "UNKNOWN"),
                    "company_name": context_data.get("company_name", "Unknown Company"),
                    "report_date": context_data.get("report_date", datetime.datetime.now().strftime("%Y-%m-%d")),
                    "analysis_start_date": context_data.get("analysis_start_date", ""),
                    "analysis_end_date": context_data.get("analysis_end_date", ""),
                    "data_source": context_data.get("data_source", "Unknown"),
                    "analysis_angles": context_data.get("analysis_angles", []),
                    "report_type": context_data.get("report_type", "Standard"),
                    "report_title": context_data.get("report_title", ""),
                    "table_of_contents": context_data.get("table_of_contents", ""),
                    "part_a_content": None,
                    "part_b_content": None,
                    "appendix_content": None
                }
            else:
                logger.warning(f"âŒ Stage 0 Validation Failed:\n{validation_msg}")
                last_error = validation_msg
                
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON Decode Error: {e}")
            last_error = f"JSON Parsing Error: {e}"
        except Exception as e:
            logger.error(f"âŒ Unexpected Error: {e}")
            last_error = str(e)
        
    # Retry with feedback
    current_prompt = f"{user_request}\n\nâš ï¸ ä¸Šä¸€æ¬¡è¼¸å‡ºæœ‰èª¤ï¼Œè«‹ä¿®æ­£:\n{last_error}\n\nè«‹å‹™å¿…è¼¸å‡ºåˆæ³•çš„ JSONï¼Œä¸¦ç¬¦åˆæ‰€æœ‰æ ¼å¼è¦æ±‚ã€‚"
    
    raise ValueError(f"Stage 0 failed after {MAX_RETRIES} attempts. Last error: {last_error}")

async def _validate_and_rewrite(stage_name: str, content: str, criteria_file: str, tool_context=None) -> Tuple[bool, str]:
    """
    é€šç”¨é©—è­‰é‚è¼¯ (Self-Correction Loop)
    1. æª¢æŸ¥å…§å®¹æ˜¯å¦ç¬¦åˆ criteria_file (é€šå¸¸æ˜¯ quality_checklist)
    2. è‹¥å¤±æ•—ï¼Œè®“ Agent é€²è¡Œä¿®æ­£
    
    Returns:
        (is_valid, content): é©—è­‰é€šéèˆ‡å¦åŠæœ€çµ‚å…§å®¹
    """
    max_retries = 5
    current_content = content
    
    for i in range(max_retries + 1):
        logger.info(f"ğŸ” Validating {stage_name} (Attempt {i+1})...")
        
        # å‰µå»ºä¸€å€‹å°ˆé–€çš„ Quality Assurance Agent
        validator = create_stage_agent(
            stage_name=f"{stage_name}_validator",
            instruction_files=["07_quality_checklist_v3_4_0.md", "01_core_principles.md"],
            include_base_instructions=False,
            description_override="ä½ æ˜¯åš´æ ¼çš„å“è³ªæª¢æŸ¥å“¡ (QA)ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæª¢æŸ¥æ¸…å–®å¯©æŸ¥å…§å®¹ï¼Œä¸¦çµ¦å‡ºé€šé(PASS)æˆ–å¤±æ•—(FAIL)çš„åˆ¤å®šã€‚"
        )
        
        # æ§‹å»ºé©—è­‰ Prompt
        validation_prompt = f"""
        è«‹é‡å°ä»¥ä¸‹å…§å®¹åŸ·è¡Œ `{criteria_file}` ä¸­çš„æª¢æŸ¥é …ç›®ï¼š

        **ç•¶å‰ç³»çµ±æ—¥æœŸ**ï¼š{datetime.datetime.now().strftime('%Y-%m-%d')}
        (è«‹å‹™å¿…æª¢æŸ¥å ±å‘Šä¸­çš„æ—¥æœŸæ˜¯å¦ç‚ºä»Šæ—¥æˆ–åˆç†çš„è¿‘æœŸæ—¥æœŸ)
        
        {current_content}
        
        è«‹åˆ¤æ–·æ˜¯å¦ç¬¦åˆè¦ç¯„ã€‚
        å¦‚æœå®Œå…¨ç¬¦åˆï¼Œè«‹åªå›ç­” "PASS"ã€‚
        å¦‚æœæœ‰ä»»ä½•ä¸ç¬¦åˆä¹‹è™•ï¼Œè«‹å›ç­” "FAIL: [å¤±æ•—åŸå› ]"ï¼Œä¸¦åˆ—å‡ºå…·é«”ä¿®æ”¹å»ºè­°ã€‚
        """
        
        
        
        
        # èª¿ç”¨ QA Agent
        validation_result = await _execute_agent_and_get_text(validator, validation_prompt, parent_context=tool_context)
        
        if "PASS" in validation_result:
            logger.info(f"âœ… {stage_name} Passed Validation.")
            return True, current_content
        else:
            logger.warning(f"âŒ {stage_name} Validation Failed: {validation_result}")
            if i < max_retries:
                logger.info(f"ğŸ”„ Attempting Self-Correction for {stage_name}...")
                
                # å‰µå»ºä¿®æ­£è€… Agent (Corrector)
                corrector = create_stage_agent(
                   stage_name=f"{stage_name}_corrector",
                   instruction_files=[criteria_file, "01_core_principles.md"], # è®“ä»–è®€é€™å€‹è¦å‰‡ä¾†æ”¹
                   include_base_instructions=False,
                   description_override="æ‚¨æ˜¯å…§å®¹ä¿®è¨‚å“¡ã€‚è«‹æ ¹æ“š QA æª¢æŸ¥å“¡çš„ä¸¦æ”¹é€²å…§å®¹ã€‚",
                   tools=[yahoo_finance_tool] # ä¿®æ­£æ™‚å¯èƒ½éœ€è¦è£œæŸ¥è³‡æ–™
                )

                rewrite_prompt = f"""
                åŸå…§å®¹å¦‚ä¸‹ï¼š
                {current_content}

                QA æª¢æŸ¥å“¡æŒ‡å‡ºä»¥ä¸‹å•é¡Œï¼š
                {validation_result}

                è«‹æ ¹æ“šä»¥ä¸Šå•é¡Œï¼Œ**ä¿®æ­£ä¸¦é‡å¯«** å®Œæ•´çš„å…§å®¹ã€‚
                è«‹ç›´æ¥è¼¸å‡ºä¿®æ­£å¾Œçš„å®Œæ•´ Markdownï¼Œä¸è¦è§£é‡‹ã€‚
                """
                
                # æ›´æ–° current_content
                current_content = await _execute_agent_and_get_text(corrector, rewrite_prompt, parent_context=tool_context)
                
    # Loop exhausted
    logger.warning(f"âš ï¸ {stage_name} failed validation after {max_retries} attempts.")
    return False, current_content

# ============================================================================
# Stage 0.5: Mandatory Data Collection (Tool Usage Enforcement)
# ============================================================================

async def _run_stage_0_5_data_collection(context: AnalysisContext, tool_context=None) -> dict:
    """
    Stage 0.5: å¼·åˆ¶å‰ç½®æ•¸æ“šæ”¶é›†
    
    æ ¹æ“š Stage 0 çš„ TOC è¦åŠƒï¼Œé å…ˆæ”¶é›†æ‰€æœ‰å¿…è¦çš„çœŸå¯¦æ•¸æ“šï¼Œ
    ç¢ºä¿å¾ŒçºŒå¯«ä½œéšæ®µä¸æœƒç”¢ç”Ÿå¹»è¦º (Hallucination)ã€‚
    
    æ‰€æœ‰å·¥å…·å‘¼å«æœƒè¨˜éŒ„åˆ°æª”æ¡ˆä¸­ä¾›é©—è­‰ï¼Œä¸è¼¸å‡ºè‡³ consoleã€‚
    """
    import os
    from datetime import datetime
    
    # å»ºç«‹ Log ç›®éŒ„
    log_dir = os.path.join(os.path.dirname(__file__), ".adk", "data_collection_logs")
    os.makedirs(log_dir, exist_ok=True)
    
    # å»ºç«‹æœ¬æ¬¡åŸ·è¡Œçš„ Log æª”æ¡ˆ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"data_collection_{context['ticker']}_{timestamp}.log")
    
    def log_tool_call(tool_name: str, source_url: str, raw_data: str, status: str = "SUCCESS"):
        """è¨˜éŒ„å·¥å…·å‘¼å«è©³æƒ…åˆ°æª”æ¡ˆ"""
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(f"\n{'='*80}\n")
            f.write(f"[{datetime.now().isoformat()}] Tool Call: {tool_name}\n")
            f.write(f"Status: {status}\n")
            f.write(f"Source: {source_url}\n")
            f.write(f"Raw Data:\n{raw_data}\n")
    
    logger.info(f"ğŸ” Starting Stage 0.5: Data Collection (Log: {log_file})")
    
    ticker = context['ticker']
    toc = context.get('table_of_contents', '')
    
    # è¨˜éŒ„æŸ¥è©¢æ™‚é–“
    from datetime import datetime
    fetch_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    data_bundle = {
        'log_file': log_file,  # ä¾›å¾ŒçºŒé©—è­‰ä½¿ç”¨
        'fetch_timestamp': fetch_timestamp,  # è³‡æ–™æŸ¥è©¢æ™‚é–“
        'current_price': 'N/A',
        'pe_ratio': 'N/A',
        'market_cap': 'N/A',
        'financials': {},
        'analyst_reports': None,
        'data_sources': []  # è¨˜éŒ„æ‰€æœ‰è³‡æ–™ä¾†æº
    }
    
    # 1. å¿…å®šå‘¼å«ï¼šåŸºç¤è‚¡åƒ¹æ•¸æ“š (Yahoo Finance)
    try:
        from stock.tools.yahoo_finance_tool import get_stock_info
        ticker_formatted = f"{ticker}.TW" if not ticker.endswith('.TW') else ticker
        
        raw_response = get_stock_info(ticker_formatted)
        log_tool_call(
            tool_name="get_stock_info (Yahoo Finance)",
            source_url=f"https://finance.yahoo.com/quote/{ticker_formatted}",
            raw_data=raw_response[:2000]  # é™åˆ¶é•·åº¦
        )
        
        # è§£æ JSON
        import json
        stock_data = json.loads(raw_response) if isinstance(raw_response, str) else raw_response
        
        if 'info' in stock_data:
            info = stock_data['info']
            
            # âš ï¸ é—œéµé©—è­‰ï¼šæª¢æŸ¥å…¬å¸åç¨±æ˜¯å¦åŒ¹é…
            actual_company_name = info.get('longName', 'Unknown')
            expected_company_name = context.get('company_name', '')
            
            # ç°¡å–®çš„æ¨¡ç³ŠåŒ¹é…ï¼ˆæª¢æŸ¥æ˜¯å¦æœ‰å…±åŒé—œéµå­—ï¼‰
            name_match = False
            if expected_company_name:
                # ç§»é™¤å¸¸è¦‹å¾Œç¶´é€²è¡Œæ¯”å°
                expected_keywords = expected_company_name.replace('è‚¡ä»½æœ‰é™å…¬å¸', '').replace('æœ‰é™å…¬å¸', '').strip()
                if expected_keywords in actual_company_name or actual_company_name in expected_company_name:
                    name_match = True
            
            # è¨˜éŒ„é©—è­‰çµæœ
            verification_status = "âœ… PASS" if name_match else "âŒ FAIL"
            log_tool_call(
                tool_name="verify_ticker",
                source_url=f"https://finance.yahoo.com/quote/{ticker_formatted}",
                raw_data=f"""
Ticker é©—è­‰çµæœ: {verification_status}

é æœŸå…¬å¸åç¨±: {expected_company_name}
å¯¦éš›å…¬å¸åç¨±: {actual_company_name}
Ticker: {ticker_formatted}

{f"âš ï¸ è­¦å‘Šï¼šå…¬å¸åç¨±ä¸åŒ¹é…ï¼è«‹ç¢ºèª Ticker æ˜¯å¦æ­£ç¢ºã€‚" if not name_match else "âœ“ é©—è­‰é€šé"}
                """.strip(),
                status="PASS" if name_match else "WARNING"
            )
            
            if not name_match:
                logger.warning(f"âš ï¸ Ticker é©—è­‰è­¦å‘Šï¼šé æœŸ '{expected_company_name}'ï¼Œå¯¦éš›ç‚º '{actual_company_name}'")
                data_bundle['ticker_verification'] = f"WARNING: Name mismatch"
            else:
                data_bundle['ticker_verification'] = "PASS"
            
            # ç¹¼çºŒæå–æ•¸æ“š
            data_bundle['current_price'] = info.get('currentPrice', 'N/A')
            data_bundle['pe_ratio'] = info.get('trailingPE', 'N/A')
            data_bundle['market_cap'] = info.get('marketCap', 'N/A')
            
            # æ“´å……ï¼šæå–æ›´å¤šè²¡å‹™æ•¸æ“š
            data_bundle['revenue'] = info.get('totalRevenue', 'N/A')
            data_bundle['gross_margin'] = info.get('grossMargins', 'N/A')
            data_bundle['ebitda'] = info.get('ebitda', 'N/A')
            data_bundle['operating_cash_flow'] = info.get('operatingCashflow', 'N/A')
            data_bundle['revenue_growth'] = info.get('revenueGrowth', 'N/A')
            data_bundle['debt_to_equity'] = info.get('debtToEquity', 'N/A')
            
            # è¨˜éŒ„å®Œæ•´å¯ç”¨æ¬„ä½ï¼ˆä¾›é©—è­‰ï¼‰
            available_keys = list(info.keys())
            log_tool_call(
                tool_name="get_stock_info (Available Fields)",
                source_url=f"https://finance.yahoo.com/quote/{ticker_formatted}",
                raw_data=f"Available data fields: {', '.join(available_keys[:50])}"  # å‰50å€‹æ¬„ä½
            )
            
            data_bundle['financials'] = stock_data.get('financials', {})
            
            # è¨˜éŒ„è³‡æ–™ä¾†æº
            data_bundle['data_sources'].append({
                'name': 'Yahoo Finance',
                'url': f'https://finance.yahoo.com/quote/{ticker_formatted}',
                'data_types': ['è‚¡åƒ¹', 'å¸‚ç›ˆç‡', 'å¸‚å€¼', 'è²¡å‹™æ•¸æ“š'],
                'fetch_time': fetch_timestamp
            })
        
    except Exception as e:
        log_tool_call(
            tool_name="get_stock_info",
            source_url="N/A",
            raw_data=f"ERROR: {str(e)}",
            status="FAILED"
        )
    
    # 2. æ¢ä»¶å‘¼å«ï¼šæœ€æ–°è²¡å ±æ–°èèˆ‡åˆ¸å•†å ±å‘Š
    if 'ä¼°å€¼' in toc or 'ç›®æ¨™åƒ¹' in toc or 'è²¡å‹™' in toc:
        logger.info("ğŸ“Š Searching for financial news and analyst reports...")
        
        company_name = context.get('company_name', ticker)
        
        # 2.1 æœå°‹æœ€æ–°è²¡å ±æ–°è
        try:
            search_query = f"{company_name} è²¡å ± 2025 Q4"
            logger.info(f"ğŸ” Searching: {search_query}")
            
            # è¼‰å…¥ MCP æœå°‹å·¥å…·
            mcp_tools = load_mcp_tools()
            
            if mcp_tools:
                # å»ºç«‹å°ˆç”¨æœå°‹ Agent
                search_agent = create_stage_agent(
                    stage_name="stage_0_5_search",  # ä¿®æ­£ï¼šä¸èƒ½æœ‰å°æ•¸é»
                    instruction_files=[],
                    include_base_instructions=False,
                    description_override="ä½ æ˜¯æœå°‹åŠ©æ‰‹ï¼Œè² è²¬æŸ¥è©¢è²¡ç¶“æ–°èã€‚",
                    tools=mcp_tools
                )
                
                # åŸ·è¡Œæœå°‹
                search_prompt = f"è«‹æœå°‹ï¼š{search_query}"
                search_results = await _execute_agent_and_get_text(search_agent, search_prompt)
                
                log_tool_call(
                    tool_name="search_web (Financial News)",
                    source_url=f"Query: {search_query}",
                    raw_data=f"Search Results: {search_results[:1000]}",
                    status="SUCCESS"
                )
                
                data_bundle['financial_news'] = search_results
            else:
                data_bundle['financial_news'] = 'N/A (MCP tools not loaded)'
                log_tool_call(
                    tool_name="search_web",
                    source_url="N/A",
                    raw_data="MCP tools not available",
                    status="SKIPPED"
                )
                
        except Exception as e:
            logger.warning(f"âš ï¸ Search failed: {e}")
            log_tool_call(
                tool_name="search_web",
                source_url="N/A",
                raw_data=f"ERROR: {str(e)}",
                status="FAILED"
            )
            data_bundle['financial_news'] = 'N/A'
        
        # 2.2 æœå°‹åˆ¸å•†ç›®æ¨™åƒ¹
        data_bundle['analyst_reports'] = "N/A (æŸ¥ç„¡å…¬é–‹åˆ¸å•†é æ¸¬æ•¸æ“š)"
    
    logger.info(f"âœ… Stage 0.5 Complete. Collected: Price={data_bundle['current_price']}, P/E={data_bundle['pe_ratio']}, Revenue={data_bundle.get('revenue', 'N/A')}")
    return data_bundle

async def _run_stage_1(context: AnalysisContext, tool_context=None) -> str:
    """Stage 1: æ·±åº¦åˆ†æ (Part A)"""
    logger.info("ğŸš€ Starting Stage 1: Part A Generation")
    
    # Load tools for real-time data access
    stage_tools = load_mcp_tools() + [yahoo_finance_tool]
    
    agent = create_stage_agent(
        stage_name="stage_1_part_a",
        instruction_files=["04_part_a_writing_guide.md", "01_core_principles.md"],
        include_base_instructions=False,
        description_override="æ‚¨æ˜¯è³‡æ·±çš„è‚¡ç¥¨åˆ†æå¸«ï¼Œè² è²¬æ’°å¯«æ·±åº¦åˆ†æå ±å‘Š (Part A)ã€‚æ‚¨å¿…é ˆä½¿ç”¨å·¥å…·æŸ¥è©¢æœ€æ–°è‚¡åƒ¹èˆ‡å¸‚å ´è³‡è¨Šã€‚",
        tools=stage_tools
    )
    
    prompt = f"""
    è«‹æ ¹æ“šä»¥ä¸‹ä¸Šä¸‹æ–‡æ’°å¯« Part A å ±å‘Šï¼š
    
    - è‚¡ç¥¨ï¼š{context['ticker']} ({context['company_name']})
    - å ±å‘Šæ—¥æœŸï¼š{context['report_date']}
    - åˆ†æè¦–è§’ï¼š{context['analysis_angles']}
    
    âš ï¸ ä»¥ä¸‹æ˜¯å·²æŸ¥è©¢çš„çœŸå¯¦æ•¸æ“šï¼Œåš´ç¦è‡ªè¡Œå‰µé€ ä»»ä½•æ•¸å­— âš ï¸
    
    **åŸºç¤æ•¸æ“š (Yahoo Finance)**ï¼š
    - ç•¶å‰è‚¡åƒ¹ï¼š{context.get('real_data', {}).get('current_price', 'N/A')}
    - å¸‚ç›ˆç‡ (P/E)ï¼š{context.get('real_data', {}).get('pe_ratio', 'N/A')}
    - å¸‚å€¼ï¼š{context.get('real_data', {}).get('market_cap', 'N/A')}
    
    **è²¡å‹™æ•¸æ“š (Yahoo Finance)**ï¼š
    - ç‡Ÿæ”¶ (Revenue)ï¼š{context.get('real_data', {}).get('revenue', 'N/A')}
    - æ¯›åˆ©ç‡ (Gross Margin)ï¼š{context.get('real_data', {}).get('gross_margin', 'N/A')}
    - EBITDAï¼š{context.get('real_data', {}).get('ebitda', 'N/A')}
    - ç‡Ÿé‹ç¾é‡‘æµ (Operating Cash Flow)ï¼š{context.get('real_data', {}).get('operating_cash_flow', 'N/A')}
    - ç‡Ÿæ”¶å¢é•·ç‡ (Revenue Growth)ï¼š{context.get('real_data', {}).get('revenue_growth', 'N/A')}
    - è² å‚µæ¬Šç›Šæ¯” (Debt-to-Equity)ï¼š{context.get('real_data', {}).get('debt_to_equity', 'N/A')}
    
    **åˆ†æå¸«æ•¸æ“š**ï¼š
    - åˆ¸å•†é æ¸¬ï¼š{context.get('real_data', {}).get('analyst_reports', 'N/A (æŸ¥ç„¡æ•¸æ“š)')}
    
    ğŸš« åš´æ ¼è¦å‰‡ï¼š
    1. è‹¥æŸé …æ•¸æ“šç‚º "N/A"ï¼Œçµ•å°ä¸å¯ç”¨ "ç´„XX%" æˆ– "ä¼°è¨ˆXXå„„" ç­‰æ¨¡ç³Šè¡¨è¿°ã€‚
    2. è‹¥æŸå€‹ä¸»é¡Œï¼ˆå¦‚å®¢æˆ¶çµæ§‹ã€ç”¢æ¥­å¹³å‡ï¼‰å®Œå…¨æ²’æœ‰æ•¸æ“šæ”¯æ’ï¼Œ**æ•´æ®µçœç•¥ä¸å¯«**ã€‚
    3. ç¦æ­¢ç·¨é€ ä»»ä½•ã€Œè¡Œæ¥­å¹³å‡ã€ã€ã€Œä¸€èˆ¬è€Œè¨€ã€ã€ã€Œé€šå¸¸æƒ…æ³ä¸‹ã€ç­‰ç„¡ä¾†æºé™³è¿°ã€‚
    
    ğŸ“Š è³‡æ–™ä¾†æºæ¨™è¨»è¦æ±‚ï¼š
    - åœ¨ã€Œé‡è¦è¨Šæ¯ã€æ®µè½æœ€å¾Œï¼Œå¿…é ˆåŠ å…¥ä»¥ä¸‹æ ¼å¼çš„ä¾†æºèªªæ˜ï¼š
    
    ---
    ğŸ“Š è³‡æ–™ä¾†æºï¼šYahoo Finance (https://finance.yahoo.com/quote/{context['ticker']}.TW)
    è³‡æ–™æ›´æ–°æ™‚é–“ï¼š{context.get('real_data', {}).get('fetch_timestamp', 'N/A')}
    ---
    
    â›”ï¸ çµ•å°ç¦æ­¢è¼¸å‡ºä»¥ä¸‹å…§å®¹ï¼ˆæœƒèˆ‡ Stage 0 é‡è¤‡ï¼‰ï¼š
    - å ±å‘Šæ¨™é¡Œ (å·²åœ¨ Stage 0 ç”¢å‡º)
    - å ±å‘Šæ—¥æœŸã€åˆ†ææœŸé–“ (å·²åœ¨ Stage 0 ç”¢å‡º)
    - ç›®éŒ„ (å·²åœ¨ Stage 0 ç”¢å‡º)
    
    âœ… å¿…é ˆä¾åºåŒ…å«ä»¥ä¸‹ç« ç¯€ (ä½¿ç”¨ ##):
    ## é‡è¦è¨Šæ¯
    ## è©•è«–åŠåˆ†æ
    ## ä¼°å€¼èˆ‡ç›®æ¨™åƒ¹
    ## æŠ•è³‡å»ºè­°
    ## æŠ•è³‡é¢¨éšª

    âš ï¸ CRITICAL DATA INTEGRITY RULE âš ï¸
    1. **Tool First**: åœ¨æ’°å¯«ä»»ä½•å…§å®¹å‰ï¼Œå¿…é ˆå…ˆå‘¼å« `get_stock_info` å–å¾—æœ€æ–°è‚¡åƒ¹ã€‚
    2. **Zero Hallucination**: è‹¥å·¥å…·å›å‚³å¤±æ•—æˆ–æŸ¥ç„¡æ•¸æ“šï¼Œåš´ç¦è‡ªè¡Œå¡«å¯«æ•¸å­—ï¼Œå¿…é ˆæ¨™è¨» "N/A"ã€‚
    3. **No Fake Citations**: åš´ç¦ä½¿ç”¨ "AAAè­‰åˆ¸" ç­‰å‡åã€‚è‹¥æŸ¥ä¸åˆ°åˆ¸å•†é æ¸¬ï¼Œè©²è¡¨æ ¼ç•™ç©ºä¸¦è¨»æ˜ "N/A"ã€‚
    
    ğŸ“‹ æ•¸æ“šä¾†æºè²æ˜ (å¿…é ˆåœ¨å ±å‘Šæœ€å¾ŒåŠ å…¥)ï¼š
    
    ---
    ## æ•¸æ“šä¾†æºè²æ˜
    
    æœ¬å ±å‘Šæ•¸æ“šä¾†æºå¦‚ä¸‹ï¼š
    
    **1. Yahoo Finance**
    - é€£çµï¼šhttps://finance.yahoo.com/quote/{context['ticker']}.TW
    - æ•¸æ“šé¡å‹ï¼šè‚¡åƒ¹ã€å¸‚ç›ˆç‡ã€å¸‚å€¼ã€è²¡å‹™æ•¸æ“š
    - æŸ¥è©¢æ™‚é–“ï¼š{context.get('real_data', {}).get('fetch_timestamp', 'N/A')}
    
    {f"**2. ç¶²è·¯æœå°‹**\\n- æŸ¥è©¢é—œéµå­—ï¼š{context.get('company_name', '')} è²¡å ± 2025 Q4\\n- æŸ¥è©¢æ™‚é–“ï¼š{context.get('real_data', {}).get('fetch_timestamp', 'N/A')}" if context.get('real_data', {}).get('financial_news') and context.get('real_data', {}).get('financial_news') != 'N/A' else ""}
    
    ---
    """
    

    
    
    logger.info("ğŸ¤– Stage 1 Agent Executing...")
    part_a_content = await _execute_agent_and_get_text(agent, prompt, parent_context=tool_context)
    
    # åŸ·è¡Œå“è³ªé©—è­‰
    is_valid, validated_content = await _validate_and_rewrite("Part A", part_a_content, "07_quality_checklist_v3_4_0.md", tool_context=tool_context)
    
    if is_valid:
        logger.info("âœ… Stage 1 (Part A) Passed Validation.")
    else:
        logger.warning("âš ï¸ Stage 1 (Part A) Completed with Validation Warnings.")
    
    # ç¨‹å¼å¼·åˆ¶é™„åŠ æ•¸æ“šä¾†æºè²æ˜
    data_source_footer = f"""

---

## æ•¸æ“šä¾†æºè²æ˜

æœ¬å ±å‘Šæ•¸æ“šä¾†æºå¦‚ä¸‹ï¼š

**1. Yahoo Finance**  
- é€£çµï¼šhttps://finance.yahoo.com/quote/{context['ticker']}.TW  
- æ•¸æ“šé¡å‹ï¼šè‚¡åƒ¹ã€å¸‚ç›ˆç‡ã€å¸‚å€¼ã€è²¡å‹™æ•¸æ“š  
- æŸ¥è©¢æ™‚é–“ï¼š{context.get('real_data', {}).get('fetch_timestamp', 'N/A')}  

---
"""
    
    return validated_content + data_source_footer

async def _run_stage_2(context: AnalysisContext, part_a_content: str, tool_context=None) -> str:
    """Stage 2: æ‘˜è¦èˆ‡è¡¨æ ¼ (Part B)"""
    logger.info("ğŸš€ Starting Stage 2: Part B Generation")
    
    agent = create_stage_agent(
        stage_name="stage_2_part_b",
        instruction_files=["05_part_b_table_guide.md", "01_core_principles.md"],
        include_base_instructions=False,
        description_override="æ‚¨æ˜¯ç²¾æº–çš„æ•¸æ“šæ•´ç†å°ˆå“¡ï¼Œè² è²¬è£½ä½œåˆ†ææ‘˜è¦è¡¨æ ¼ (Part B)ã€‚",
        tools=[yahoo_finance_tool]
    )
    
    prompt = f"""
    è«‹æ ¹æ“šä»¥ä¸‹ Part A çš„å…§å®¹ï¼Œè£½ä½œ Part B æ‘˜è¦è¡¨æ ¼ï¼š
    
    [Part A Content Start]
    {part_a_content}
    [Part A Content End]
    
    - è‚¡ç¥¨ï¼š{context['ticker']}
    - ç›®æ¨™ï¼š
        1. å¡«å¯«ã€Œåƒ¹æ ¼èˆ‡ç›®æ¨™åƒ¹ã€è¡¨æ ¼ (æ•¸æ“šéœ€èˆ‡ Part A ä¸€è‡´)ã€‚
        2. èƒå– 4 é»ã€Œç„¦é»å…§å®¹ã€ (å¿…é ˆä¾†è‡ª Part A)ã€‚
        3. è£½ä½œã€Œäº¤æ˜“è³‡æ–™ã€èˆ‡ã€Œè‚¡åƒ¹è¡¨ç¾ã€è¡¨æ ¼ã€‚
        
        âš ï¸ é‡è¦æç¤ºï¼š
        - Part A ä¸­å¯èƒ½ç¼ºå°‘éƒ¨åˆ†äº¤æ˜“æ•¸æ“šï¼ˆå¦‚å¸‚å€¼ã€æµé€šè‚¡æ•¸ã€3M Avg Volume ç­‰ï¼‰ã€‚
        - è‹¥ç™¼ç¾æ•¸æ“šç¼ºå¤±ï¼Œè«‹ **ç«‹å³ä½¿ç”¨ `yahoo_finance_tool`** æŸ¥è©¢è£œè¶³ã€‚
        - åš´ç¦ç•™ä¸‹ "N/A"ï¼Œé™¤éå·¥å…·ä¹ŸæŸ¥ä¸åˆ°ã€‚
    
    è«‹åš´æ ¼éµå¾ª `05_part_b_table_guide` çš„æ ¼å¼ã€‚
    """
    

    
    
    logger.info("ğŸ¤– Stage 2 Agent Executing...")
    part_b_content = await _execute_agent_and_get_text(agent, prompt, parent_context=tool_context)
    
    # åŸ·è¡Œå“è³ªé©—è­‰
    is_valid, validated_content = await _validate_and_rewrite("Part B", part_b_content, "07_quality_checklist_v3_4_0.md", tool_context=tool_context)
    
    if is_valid:
        logger.info("âœ… Stage 2 (Part B) Passed Validation.")
    else:
        logger.warning("âš ï¸ Stage 2 (Part B) Completed with Validation Warnings.")
        
    return validated_content


    
async def _run_stage_3(context: AnalysisContext, tool_context=None) -> str:
    """Stage 3: é™„éŒ„èˆ‡çµ„è£ (Appendix)"""
    logger.info("ğŸš€ Starting Stage 3: Appendix Generation")
    
    agent = create_stage_agent(
        stage_name="stage_3_appendix",
        instruction_files=["06_appendix_reference_guide.md", "02_data_source_selection.md", "01_core_principles.md"],
        include_base_instructions=False,
        description_override="æ‚¨æ˜¯åš´è¬¹çš„æ–‡æª”ç®¡ç†å“¡ï¼Œè² è²¬è£½ä½œé™„éŒ„èˆ‡åƒè€ƒæ–‡ç»ã€‚"
    )
    
    prompt = f"""
    è«‹æ ¹æ“šæœ¬æ¬¡åˆ†æä½¿ç”¨çš„æ•¸æ“šæºèˆ‡è¦ç¯„ï¼Œè£½ä½œå ±å‘Šé™„éŒ„ï¼š
    
    - æ•¸æ“šæºé…ç½®ï¼š{context['data_source']}
    - å ±å‘Šæ—¥æœŸï¼š{context['report_date']}
    - æ•¸æ“šæˆªæ­¢æ—¥ï¼š{context['analysis_end_date']}
    
    ä»»å‹™ï¼š
    1. è£½ä½œã€Œæ•¸æ“šä¾†æºèˆ‡å…è²¬è²æ˜ã€ (å«é•·ç‰ˆæ³•å¾‹è²æ˜)ã€‚
    2. è£½ä½œã€Œåƒè€ƒæ–‡ç»èˆ‡æ•¸æ“šä¾†æºã€è¡¨æ ¼ (åš´æ ¼éµå¾ª `06_appendix` æ ¼å¼ï¼Œéœ€åˆ—å‡º Morningstarã€è²¡å ±ç­‰å…·é«”é …ç›®)ã€‚
    
    è«‹æ³¨æ„ï¼šPart B ä¸‹æ–¹å·²åŒ…å«çŸ­ç‰ˆè²æ˜ï¼Œæ­¤è™•ç‚º **å®Œæ•´é™„éŒ„**ã€‚
    """
    

    
    
    logger.info("ğŸ¤– Stage 3 Agent Executing...")
    appendix_content = await _execute_agent_and_get_text(agent, prompt, parent_context=tool_context)
    
    # åŸ·è¡Œå“è³ªé©—è­‰
    is_valid, validated_content = await _validate_and_rewrite("Appendix", appendix_content, "07_quality_checklist_v3_4_0.md", tool_context=tool_context)
    
    if is_valid:
        logger.info("âœ… Stage 3 (Appendix) Passed Validation.")
    else:
        logger.warning("âš ï¸ Stage 3 (Appendix) Completed with Validation Warnings.")
        
    return validated_content

async def run_analysis_pipeline(user_request: str, tool_context=None):
    """
    åŸ·è¡Œå®Œæ•´åˆ†ææµæ°´ç·š
    """
    logger.info("ğŸ”¥ Initializing Analysis Pipeline...")

    # æ¸…ç©º debug log
    try:
        with open("latest_debug_prompt.txt", "w", encoding="utf-8") as f:
             f.write(f"Pipeline Started at {datetime.datetime.now()}\n")
    except:
        pass
    
    
    # Stage 0
    context = await _run_stage_0(user_request, tool_context=tool_context)
    logger.info(f"âœ… Stage 0 Complete. Context: {context}")
    
    # Stage 0.5: Mandatory Data Collection
    real_data = await _run_stage_0_5_data_collection(context, tool_context=tool_context)
    context['real_data'] = real_data
    logger.info(f"âœ… Stage 0.5 Complete. Data Log: {real_data.get('log_file')}")
    
    # [TEST MODE] Skipping Stages 2-3 for Part A Verification
    logger.info("ğŸš§ [TEST MODE] Skipping Stage 2, 3. Using placeholders.")
    
    # Stage 1 (Part A)
    context['part_a_content'] = await _run_stage_1(context, tool_context=tool_context)
    # context['part_a_content'] = "### (Part A Skipped for Testing)"
    logger.info("âœ… Stage 1 (Part A) Complete.")
    
    # Stage 2 (Part B)
    # context['part_b_content'] = await _run_stage_2(context, context['part_a_content'], tool_context=tool_context)
    context['part_b_content'] = "### (Part B Skipped for Testing)"
    logger.info("âœ… Stage 2 (Part B) Skipped.")
    
    # Stage 3 (Appendix)
    # context['appendix_content'] = await _run_stage_3(context, tool_context=tool_context)
    context['appendix_content'] = "### (Appendix Skipped for Testing)"
    logger.info("âœ… Stage 3 (Appendix) Skipped.")
    
    # Final Assembly
    logger.info("ğŸ“¦ Assembling Final Report...")
    
    # æ§‹å»ºåŒ…å«æ¨™é¡Œã€ç›®éŒ„ã€å„éƒ¨åˆ†å…§å®¹çš„å®Œæ•´å ±å‘Š
    title = context.get('report_title', f"# {context.get('company_name', 'Unknown')} åˆ†æå ±å‘Š")
    toc = context.get('table_of_contents', "")

    final_report = f"""
{title}

å ±å‘Šæ—¥æœŸ: {context.get('report_date')}
åˆ†ææœŸé–“: {context.get('analysis_start_date')} - {context.get('analysis_end_date')}
è³‡æ–™ä¾†æº: {context.get('data_source')}

---

{toc}

---

## Part A: æ·±åº¦åˆ†æå ±å‘Š

{context['part_a_content']}

---

## Part B: é‡é»æ‘˜è¦è¡¨æ ¼

{context['part_b_content']}

---

## é™„éŒ„ (Appendix)

{context['appendix_content']}
    """
    
    logger.info("ğŸ‰ Analysis Pipeline Completed Successfully!")
    
    # [Clean Text Policy] å¼·åˆ¶ç§»é™¤æ‰€æœ‰ Markdown Code Block æ¨™è¨˜ (```)
    # å…ˆç§»é™¤èªè¨€æ¨™è¨˜ï¼Œå†ç§»é™¤ backticksï¼Œé¿å…ç•™ä¸‹ stray text
    final_report = final_report.replace("```markdown", "").replace("```json", "").replace("```", "").strip()
    
    return final_report

# å°‡ Pipeline åŒ…è£ç‚ºå·¥å…·
pipeline_tool = FunctionTool(run_analysis_pipeline)

# ============================================================================
# Root Agent Configuration (Tool Swapping Strategy)
# ============================================================================

# æˆ‘å€‘ä¸å†ä½¿ç”¨è‡ªå®šç¾© Classï¼Œè€Œæ˜¯ä½¿ç”¨æ¨™æº– Agentï¼Œä½†åªçµ¦å®ƒä¸€å€‹å·¥å…·ï¼šPipeline Tool
# ä¸¦é€é System Prompt å¼·åˆ¶å®ƒä½¿ç”¨é€™å€‹å·¥å…·

root_agent = Agent(
    model=LiteLlm(model="azure/gpt-4o"),
    name="stock_analyst",
    description="Stock Analyst Agent",
    # åªæä¾› Pipeline å·¥å…·ï¼Œå¼·è¿« Agent é€²å…¥æˆ‘å€‘çš„ Python é‚è¼¯
    tools=[pipeline_tool], 
    static_instruction="""
    æ‚¨æ˜¯è‚¡ç¥¨åˆ†æå ±å‘Šç”Ÿæˆå™¨çš„å…¥å£ã€‚
    
    **å”¯ä¸€ä»»å‹™**ï¼š
    ç•¶æ”¶åˆ°ä»»ä½•è‚¡ç¥¨ä»£è™Ÿæˆ–å…¬å¸åç¨±æ™‚ï¼ˆä¾‹å¦‚ "AMD", "TSMC", "åˆ†æ Apple"ï¼‰ï¼Œ
    æ‚¨å¿…é ˆ**ç«‹å³**èª¿ç”¨ `run_analysis_pipeline` å·¥å…·ã€‚
    
    **è¼¸å‡ºè¦å‰‡**ï¼š
    è©²å·¥å…·æœƒè¿”å›å®Œæ•´çš„ç¹é«”ä¸­æ–‡åˆ†æå ±å‘Šã€‚
    æ‚¨å¿…é ˆ**åŸå°ä¸å‹•**åœ°å°‡å·¥å…·çš„è¼¸å‡ºå‘ˆç¾çµ¦ç”¨æˆ¶ã€‚
    
    - âŒ ç¦æ­¢è‡ªè¡Œæ’°å¯«æ‘˜è¦ã€‚
    - âŒ ç¦æ­¢ä½¿ç”¨è‡ªå·±çš„çŸ¥è­˜åº«å›ç­”ã€‚
    - âœ… å¿…é ˆä¸”åªèƒ½å‘¼å« `run_analysis_pipeline`ã€‚
    """,
    include_contents='none'
)



# ============================================================================
# é–‹ç™¼å·¥å…·å‡½æ•¸
# ============================================================================

def show_loaded_modules():
    """é¡¯ç¤ºç•¶å‰å·²è¼‰å…¥çš„æ¨¡çµ„è³‡è¨Š"""
    instruction_dir = os.path.join(os.path.dirname(__file__), "instructions")
    modules = extract_modules_from_instructions(instruction_dir)
    
    logger.info("\n" + "="*60)
    logger.info(f"ğŸ“¦ å·²è¼‰å…¥ {len(modules)} å€‹æ¨¡çµ„")
    logger.info("="*60)
    
    for module_id, info in sorted(modules.items(), key=lambda x: x[1]['order']):
        logger.info(f"\n{info['order']:02d}. {info['name']} ({module_id})")
        logger.info(f"    æ–‡ä»¶: {info['file']}")
        if info['aliases']:
            aliases_str = 'ã€'.join(f'"{a}"' for a in info['aliases'][:5])
            logger.info(f"    åˆ¥å: {aliases_str}")
        if info['description']:
            logger.info(f"    èªªæ˜: {info['description']}")
    
    logger.info("\n" + "="*60 + "\n")


# [DEBUG] Force run load_instructions on module import to ensure logging
logger.info("[DEBUG] Agent module initialized, forcing instruction load + logging...")
load_instructions()
